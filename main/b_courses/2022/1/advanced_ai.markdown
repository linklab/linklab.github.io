---
layout: page
permalink: /courses/2022/1/advanced_ai
---

<section style="overflow-wrap: anywhere; word-wrap: anywhere;">
    <div class="cw-content container-fluid">
        <div class="cyw-container" style="width: 100%; margin-left: auto; margin-right: auto">
            <div class="container" style="width: 100%; margin-left: auto; margin-right: auto">
                <!--Start Container Div-->
                <div style="background-color:white;" class="container-fluid">
                    <!--Start Content Grid-->
                    <div class="row content">
                        <div class="content-fluid">
                            <div class="cw-content container-fluid">
                                <div class="cyw-container">
                                    <div class="container">
                                        <!--Start Container Div-->
                                        <div style="background-color:white;color:black" class="container-fluid">
                                            <!--Start Content Grid-->
                                            <div class="row content">
                                                <div class="content-wrapper">
                                                    <h2 class="title-level-2">
                                                        Advanced Artificial Intelligence (인공지능 특강 [240030-01], Spring Semester, 2022) </h2>
                                                    <p><em><br>“Student-professor relationships are based on trust. Acts,
                                                        which violate this trust, undermine the educational process.
                                                        Your classmates and the professor will not tolerate violations
                                                        of academic integrity.”</em></p><br>
                                                    <h3 class="title-level-3">1. Course Schedule &amp; Lecture Notes</h3>
                                                    <br/>
                                                    <div>
                                                        <h3>[공지사항]</h3>
                                                        <i class="fas fa-bullhorn"></i> [2022.02.13] 본 수업을 추가적으로 신청하고자 하는 학생(또는 이미 수강신청을 완료한 학생)들에게 공지합니다. 본 수업은 심층강화학습을 뼈대가 되는 주요 내용 및 알고리즘을 15번의 수업에 걸쳐서 학습하는 내용을 담고 있습니다. 모든 학생들은 1) 아래 열거된 핵심 논문의 일부 내용을 공부하고 자료를 구성하여 직접 수업시간에 여러 대학원생들 앞에서 발표를 해야 하며, 2) 수업시간에 소개하는 LINK_RL 프레임워크를 이해하고 이를 기반으로 주요 심층강화학습 알고리즘을 이해하고 이를 기반으로 3번에 걸친 코딩 숙제 및 실험 평가 분석을 하여 리포트를 제출해야 하며, 3) 기말고사를 통하여 수업에서 다룬 심층강화학습 내용 및 알고리즘 전반에 걸친 이해도를 평가받게 됩니다.
                                                        <br/>
                                                        <i class="fas fa-bullhorn"></i> [2022.02.13] 본 수업을 수강하기 위한 꼭 필요한 선수 지식: 1) 학부과정에서 알고리즘 등의 과목을 통하여 또는 과제/업무 수행을 통하여 컴퓨터 프로그래밍 기반으로 주어진 문제를 해결해본 경험, 2) 파이썬을 통하여 Tensorflow 또는 Pytorch 기반으로 딥러닝 관련 코딩 수행 경험, 3) 파이썬을 통한 가상 환경 구축 및 라이브러리 구축 경험
                                                        <br/>
                                                        <i class="fas fa-bullhorn"></i> [2022.02.13] 학점은 A+/A, B+/B, C+/C, F 이렇게 총 4개의 그룹으로 나누어 부여할 예정이며, F로 평가될 학생이 없다면 각 3개 그룹당 학점의 분포는 40%, 40%, 20%로 나누어 부여할 예정이지만 강의가 종료된 이후 전반적인 학업성취도를 가늠하여 변경될 수 있습니다.
                                                        <br/>
                                                        <i class="fas fa-bullhorn"></i> [2022.02.11] 강의에서 사용하는 심층강화학습 프레임워크 Github Repository: <a href="https://github.com/linklab/link_rl" target="_blank"><i class="fa fa-github-square" aria-hidden="true"></i></a>
                                                    </div>
                                                    <br/>
                                                    <table class="table table-responsive table-hover">
                                                        <thead class="thead-light">
                                                        <tr>
                                                            <th scope="col" style="width:4%">#</th>
                                                            <th scope="col" style="width:10%">Date</th>
                                                            <th scope="col" style="width:36%">Paper Presentation</th>
                                                            <th scope="col" style="width:30%">Lecture Subject</th>
                                                            <th scope="col" style="width:20%">Notice</th>
                                                        </tr>
                                                        </thead>
                                                        <tbody>
                                                        <tr>
                                                            <th scope="row">01</th>
                                                            <td>03월 08일(화)</td>
                                                            <td>
                                                                - 강의 소개<br/>
                                                                - Introduction to Entropy, Cross Entropy, KL Divergence, Likelihood, Maximum Likelihood Estimation (MLE), and Maximum a Posteriori Estimation (MAP)
                                                                <br/>
                                                                <a href="https://www.dropbox.com/s/vwg6kbpls7iq35k/RL-1_entropy.pdf?dl=0" target="_blank">
                                                                    <span class="badge badge-warning">강의 노트</span>
                                                                </a>
                                                            </td>
                                                            <td>
                                                                - <a href="https://github.com/linklab/link_rl" target="_blank">link_rl 프레임워크</a> 사용법 소개
                                                            </td>
                                                            <td>-</td>
                                                        </tr>
                                                        <tr>
                                                            <th scope="row">02</th>
                                                            <td>03월 15일(화)</td>
                                                            <td colspan="2">
                                                                - Introduction to Deep Reinforcement Learning (1/4)
                                                                <br/>
                                                                <a href="https://www.dropbox.com/s/r8ltsen5n4a7tud/RL-2_fundamental_1.pdf?dl=0" target="_blank">
                                                                    <span class="badge badge-warning">강의 노트</span>
                                                                </a>
                                                                <br/>
                                                                - Introduction to Deep Reinforcement Learning (2/4)
                                                                <br/>
                                                                <a href="https://www.dropbox.com/s/va4q2laz194c37y/RL-3_fundamental_2.pdf?dl=0" target="_blank">
                                                                    <span class="badge badge-warning">강의 노트</span>
                                                                </a>
                                                            </td>
                                                            <td>
                                                                <span class="font-weight-bold">
                                                                    Homework #1
                                                                </span>. Gym Cartpole & Lunarlander control by using DQN agent on the 'link_rl' framework (Due Date: April 6, 23:59:59)
                                                                <a href="https://www.dropbox.com/s/b6epuzio0jtkwcc/ADVANCED_AI_HW_1.pdf?dl=0">
                                                                    <span class="badge badge-warning badge-important">Guide for Homework #1</span>
                                                                </a>
                                                            </td>
                                                        </tr>
                                                        <tr>
                                                            <th scope="row">03</th>
                                                            <td>03월 22일(화)</td>
                                                            <td>
                                                                - Introduction to Deep Reinforcement Learning (3/4)
                                                                <br/>
                                                                <a href="https://www.dropbox.com/s/n24y1v1m7o2putp/RL-4_fundamental_3.pdf?dl=0" target="_blank">
                                                                    <span class="badge badge-warning">강의 노트</span>
                                                                </a>
                                                                <br/>
                                                                - Introduction to Deep Reinforcement Learning (4/4)
                                                                <br/>
                                                                <a href="https://www.dropbox.com/s/9jaa8f6qeayhdum/RL-5_fundamental_4.pdf?dl=0" target="_blank">
                                                                    <span class="badge badge-warning">강의 노트</span>
                                                                </a>
                                                            </td>
                                                            <td>
                                                                - <a href="https://github.com/linklab/link_rl/tree/main/h_etc/table_q_learning" target="_blank" class="font-weight-bold">Tabular Q-Learning</a> 
                                                            </td>
                                                            <td></td>
                                                        </tr>
                                                        <tr>
                                                            <th scope="row">04</th>
                                                            <td>03월 29일(화)</td>
                                                            <td>
                                                                - Volodymyr Mnih et. al., "<a href="https://arxiv.org/abs/1312.5602" target="_blank" class="font-weight-bold">Playing Atari with Deep Reinforcement Learning</a>," NIPS 2013. (발표자: 유연휘, 한호준)
                                                                <a href="https://www.dropbox.com/s/l0o6risv96d9fvt/dqn.pdf?dl=0" target="_blank"><span class="badge badge-info">발표 자료</span></a>
                                                            </td>
                                                            <td>
                                                                - <a href="https://github.com/linklab/link_rl/blob/main/d_agents/off_policy/dqn/agent_dqn.py" target="_blank" class="font-weight-bold">DQN</a> (Deep Q-Networks) <span class="badge badge-success">OFF-POLICY</span>
                                                                <a href="https://www.dropbox.com/s/ipx8s62rln4he2s/RL-6_DQN.pdf?dl=0" target="_blank">
                                                                    <span class="badge badge-warning">강의 노트</span>
                                                                </a>
                                                            </td>
                                                            <td></td>
                                                        </tr>
                                                        <tr>
                                                            <th scope="row">05</th>
                                                            <td>04월 5일(화)</td>
                                                            <td>
                                                                - Hado van Hasselt, Arthur Guez, and David Silver, "<a href="https://arxiv.org/abs/1509.06461" target="_blank" class="font-weight-bold">Deep Reinforcement Learning with Double Q-learning</a>," AAAI 2016. (발표자: 최대준, 최상원)
                                                                <a href="https://www.dropbox.com/s/6ix4gb814b454fe/double_dqn.pdf?dl=0" target="_blank"><span class="badge badge-info">발표 자료</span></a>
                                                                <br/>
                                                                - Ziyu Wang, et. al., "<a href="https://arxiv.org/abs/1511.06581" target="_blank" class="font-weight-bold">Dueling Network Architectures for Deep Reinforcement Learning</a>," ICML 2016. (발표자: 김규령, 유승관, 최요한)
                                                                <a href="https://www.dropbox.com/s/xqnyvk0i2dja0tv/dueling_dqn.pdf?dl=0" target="_blank"><span class="badge badge-info">발표 자료</span></a>
                                                            </td>
                                                            <td>
                                                                - <a href="https://github.com/linklab/link_rl/blob/main/d_agents/off_policy/dqn/agent_double_dqn.py" target="_blank" class="font-weight-bold">Double DQN</a> <span class="badge badge-success">OFF-POLICY</span>
                                                            <br/>
                                                                - <a href="https://github.com/linklab/link_rl/blob/main/d_agents/off_policy/dqn/agent_dueling_dqn.py" target="_blank" class="font-weight-bold">Dueling DQN</a> <span class="badge badge-success">OFF-POLICY</span>
                                                            <br/>
                                                                - <a href="https://github.com/linklab/link_rl/blob/main/d_agents/off_policy/dqn/agent_double_dueling_dqn.py" target="_blank" class="font-weight-bold">Double Dueling DQN</a> <span class="badge badge-success">OFF-POLICY</span></td>
                                                            <td>
{#                                                                <span class="font-weight-bold">#}
{#                                                                    Homework #1#}
{#                                                                </span>. <a href="https://gym.openai.com/envs/Pong-v0/" target="_blank">Gym Pong</a> 게임 제어 에이전트 학습후 결과 그래프를 포함한 리포트 제출#}
                                                            </td>
                                                        </tr>
                                                        <tr>
                                                            <th scope="row">06</th>
                                                            <td>04월 12일(화)</td>
                                                            <td>
                                                                - Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver, "<a href="https://arxiv.org/abs/1511.05952" target="_blank" class="font-weight-bold">Prioritized Experience Replay</a>," ICLR 2016. (발표자: 권준형, 김범영)
                                                                <a href="https://www.dropbox.com/s/ysdtf4hnx1bgncq/per.pdf?dl=0" target="_blank"><span class="badge badge-info">발표 자료</span></a>
                                                            </td>
                                                            <td>
                                                                - <a href="https://github.com/linklab/link_rl/blob/d5e16330b8403c4e675c62d1c7206fe35b901ee4/g_utils/prioritized_buffer.py#L70" target="_blank" class="font-weight-bold">PER (Prioritized Experience Replay)</a>
                                                                <a href="https://www.dropbox.com/s/8g4m4bg3fwn2v1t/RL-6_2_PER.pdf?dl=0" target="_blank">
                                                                    <span class="badge badge-warning">강의 노트</span>
                                                                </a>
                                                            </td>
                                                            <td>
                                                                <span class="font-weight-bold">
                                                                    Homework #2
                                                                </span>. Pong control by using four DQN algorithms on the 'link_rl' framework (Due Date: April 30, SAT, 23:59:59)
                                                                <a href="https://www.dropbox.com/s/7gm4co9pp4noz96/ADVANCED_AI_HW_2.pdf?dl=0">
                                                                    <span class="badge badge-warning badge-important">Guide for Homework #2</span>
                                                                </a>
                                                            </td>
                                                        </tr>
                                                        <tr>
                                                            <th scope="row">07</th>
                                                            <td>04월 19일(화)</td>
                                                            <td>
                                                                - Chapter 2. BACKGROUND in "<a href="http://joschu.net/docs/thesis.pdf" target="_blank" class="font-weight-bold">Optimizing Expectations: From Deep Reinforcement Learning to Stochastic Computation Graphs</a>," Schulman, University of California, Berkeley, 2016. (발표자: 김현재, 상희민, 석영준)
                                                                <a href="https://www.dropbox.com/s/tsmhkhmlxfsb1y8/pg.pdf?dl=0" target="_blank"><span class="badge badge-info">발표 자료</span></a>
                                                            </td>
                                                            <td>
                                                                - PG (Policy Gradient) & <a href="https://github.com/linklab/link_rl/blob/main/d_agents/on_policy/reinforce/agent_reinforce.py" target="_blank" class="font-weight-bold">REINFORCE</a> <span class="badge badge-primary">ON-POLICY</span>
                                                                <a href="https://www.dropbox.com/s/l8d8epa7qy1tcn3/RL-7_Policy_Gradient_REINFORCE.pdf?dl=0" target="_blank">
                                                                    <span class="badge badge-warning">강의 노트</span>
                                                                </a>
                                                            </td>
                                                            <td></td>
                                                        </tr>
                                                        <tr>
                                                            <th scope="row">08</th>
                                                            <td>04월 26일(화)</td>
                                                            <td>
                                                                - Volodymyr Mnih et. al., "<a href="https://arxiv.org/abs/1602.01783" target="_blank" class="font-weight-bold">Asynchronous Methods for Deep Reinforcement Learning</a>," ICML 2016. (발표자: 김성현, 이재승)
                                                                <a href="https://www.dropbox.com/s/3d3gkwc2kpqdev4/a2c_a3c.pdf?dl=0" target="_blank"><span class="badge badge-info">발표 자료</span></a>
                                                            </td>
                                                            <td>
                                                                - <a href="https://github.com/linklab/link_rl/blob/main/d_agents/on_policy/a2c/agent_a2c.py" target="_blank" class="font-weight-bold">Discrete A2C (Advantage Actor-Critic)</a> <span class="badge badge-primary">ON-POLICY</span>
                                                                <a href="https://www.dropbox.com/s/slov3638zwesdhh/RL-8_A2C_A3C.pdf?dl=0" target="_blank">
                                                                    <span class="badge badge-warning">강의 노트</span>
                                                                </a>
                                                                &
                                                                <br/>
                                                                - <a href="https://github.com/linklab/link_rl/blob/main/d_agents/on_policy/a3c/agent_a3c.py" target="_blank" class="font-weight-bold">Discrete A3C (Asynchronous Advantage Actor-Critic)</a> <span class="badge badge-primary">ON-POLICY</span></td>
                                                            <td></td>
                                                        </tr>
                                                        <tr>
                                                            <th scope="row">09</th>
                                                            <td>05월 03일(화)</td>
                                                            <td>
                                                                - John Schulman et. al., "<a href="https://arxiv.org/abs/1707.06347" target="_blank" class="font-weight-bold">Proximal Policy Optimization Algorithms</a>," CoRR, abs/1707.06347, 2017. (발표자: 박효경, 이서영)
                                                                <a href="https://www.dropbox.com/s/w47ov25b9oxx3ci/ppo.pdf?dl=0" target="_blank"><span class="badge badge-info">발표 자료</span></a>
                                                            </td>
                                                            <td>
                                                                - <a href="https://github.com/linklab/link_rl/blob/main/d_agents/on_policy/ppo/agent_ppo.py" target="_blank" class="font-weight-bold">Discrete PPO (Proximal Policy Gradient)</a> <span class="badge badge-primary">ON-POLICY</span>
                                                                <a href="https://www.dropbox.com/s/ja1xv5brez6vwkr/Policy_Gradient.pdf?dl=0" target="_blank">
                                                                    <span class="badge badge-warning">강의 노트 [New & Complete @ 2022]</span>
                                                                </a>
                                                            </td>
                                                            <td>
{#                                                                <span class="font-weight-bold">Homework #2</span>. <a href="https://gym.openai.com/envs/Pong-v0/" target="_blank">Inverted Double Pendulum</a> A2C, A3C, PPO 제어 성능 비교 실험 및 결과 그래프 담은 리포트 제출#}
                                                            </td>
                                                        </tr>
                                                        <tr>
                                                            <th scope="row">10</th>
                                                            <td>05월 10일(화)</td>
                                                            <td>
                                                                - John Schulman et. al., "<a href="https://arxiv.org/abs/1506.02438" target="_blank" class="font-weight-bold">High-Dimensional Continuous Control Using Generalized Advantage Estimation</a>", ICLR 2016. (발표자: 김회창, 차민혁, 최호빈)
                                                                <a href="https://www.dropbox.com/s/l3kcrf0lkmu5f7r/trpo.pdf?dl=0" target="_blank"><span class="badge badge-info">발표 자료</span></a>
{#                                                                - Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov, "<a href="https://arxiv.org/abs/1810.12894" target="_blank" class="font-weight-bold">Exploration by Random Network Distillation</a>", ICLR 2019. (발표자: OOO, OOO)#}
                                                            </td>
                                                            <td>
                                                                - <a href="https://github.com/linklab/link_rl/blob/main/d_agents/on_policy/a2c/agent_a2c.py" target="_blank" class="font-weight-bold">Continuous A2C</a> <span class="badge badge-primary">ON-POLICY</span>
                                                                <br/>
                                                                - <a href="https://github.com/linklab/link_rl/blob/main/d_agents/on_policy/a3c/agent_a3c.py" target="_blank" class="font-weight-bold">Continuous A3C</a> <span class="badge badge-primary">ON-POLICY</span>
                                                                <br/>
                                                                - <a href="https://github.com/linklab/link_rl/blob/main/d_agents/on_policy/ppo/agent_ppo.py" target="_blank" class="font-weight-bold">Continuous PPO</a> <span class="badge badge-primary">ON-POLICY</span></td>
                                                            <td></td>
                                                        </tr>
                                                        <tr>
                                                            <th scope="row">11</th>
                                                            <td>05월 17일(화)</td>
                                                            <td>
                                                                - Timothy P. Lillicrap et. al., "<a href="https://arxiv.org/abs/1509.02971" target="_blank" class="font-weight-bold">Continuous Control with Deep Reinforcement Learning</a>," arXiv preprint arXiv:1509.02971, 2015. (발표자: 이성준, 유진)
                                                                <a href="https://www.dropbox.com/s/cea5upaq8tj4nrr/ddpg.pdf?dl=0" target="_blank"><span class="badge badge-info">발표 자료</span></a>
                                                            </td>
                                                            <td>
                                                                - <a href="https://github.com/linklab/link_rl/blob/main/d_agents/off_policy/ddpg/agent_ddpg.py" target="_blank" class="font-weight-bold">DDPG (Deep Deterministic Policy Gradient)</a> <span class="badge badge-success">OFF-POLICY</span>
                                                                <a href="https://www.dropbox.com/s/o7qwdkkjod0urjw/RL-10_DDPG.pdf?dl=0" target="_blank">
                                                                    <span class="badge badge-warning">강의 노트</span>
                                                                </a>
                                                            </td>
                                                            <td>
                                                                <span class="font-weight-bold">
                                                                    Homework #3
                                                                </span>. Bipedal Walker control by using A2C/A3C/PPO/TD3/SAC on the 'link_rl' framework (Due Date: June 11, SAT, 23:59:59)
                                                                <a href="https://www.dropbox.com/s/ok4i9cuja3fukv6/ADVANCED_AI_HW_3.pdf?dl=0">
                                                                    <span class="badge badge-warning badge-important">Guide for Homework #3</span>
                                                                </a>
                                                            </td>
                                                        </tr>
                                                        <tr>
                                                            <th scope="row">12</th>
                                                            <td>05월 24일(화)</td>
                                                            <td>
                                                                - Scott Fujimoto, Herke van Hoof, and David Meger, "<a href="https://arxiv.org/abs/1802.09477" target="_blank" class="font-weight-bold">Addressing Function Approximation Error in Actor-Critic Methods</a>," ICML 2018. (발표자: 강병창, 현주영)
                                                                <a href="https://www.dropbox.com/s/3pe7j9q0s9sgxhv/td3.pdf?dl=0" target="_blank"><span class="badge badge-info">발표 자료</span></a>
                                                            </td>
                                                            <td>
                                                                - <a href="https://github.com/linklab/link_rl/blob/main/d_agents/off_policy/td3/agent_td3.py" target="_blank" class="font-weight-bold">TD3 (Deep Deterministic Policy Gradient)</a> <span class="badge badge-success">OFF-POLICY</span>
                                                                <a href="https://www.dropbox.com/s/q9qcunkzpjyc4d1/RL-11_TD3.pdf?dl=0" target="_blank">
                                                                    <span class="badge badge-warning">강의 노트</span>
                                                                </a>
                                                            </td>
                                                            <td></td>
                                                        </tr>
                                                        <tr>
                                                            <th scope="row">13</th>
                                                            <td>05월 31일(화)</td>
                                                            <td>
                                                                - Tuomas Haarnoja et. al., "<a href="https://arxiv.org/abs/1812.05905" target="_blank" class="font-weight-bold">Soft Actor-Critic Algorithms and Applications</a>," CoRR abs/1812.05905, 2018. (발표자: 김동환, 임상훈)
                                                                <a href="https://www.dropbox.com/s/ugostt6qsg7e3xc/sac.pdf?dl=0" target="_blank"><span class="badge badge-info">발표 자료</span></a>
                                                            </td>
                                                            <td>
                                                                - <a href="https://github.com/linklab/link_rl/blob/main/d_agents/off_policy/sac/agent_sac.py" target="_blank" class="font-weight-bold">SAC (Soft Actor-Critic)</a> <span class="badge badge-success">OFF-POLICY</span>
                                                                <a href="https://www.dropbox.com/s/nf2nadkb363hi60/RL-12_SAC.pdf?dl=0" target="_blank">
                                                                    <span class="badge badge-warning">강의 노트</span>
                                                                </a>
                                                            </td>
                                                            <td></td>
                                                        </tr>
                                                        <tr>
                                                            <th scope="row">14</th>
                                                            <td>06월 07일(화)</td>
                                                            <td>
                                                                - Deepak Pathak et. al., "<a href="https://arxiv.org/abs/1705.05363" target="_blank" class="font-weight-bold">Curiosity-driven Exploration by Self-supervised Prediction</a>," ICML 2017. (발표자: 용성중, 송병진)
                                                                <a href="https://www.dropbox.com/s/ks49ztokt9f9zd7/icm.pdf?dl=0" target="_blank"><span class="badge badge-info">발표 자료</span></a>
                                                            </td>
                                                            <td>
                                                                - <a href="https://github.com/linklab/link_rl/blob/main/d_agents/off_policy/sac/agent_sac.py" target="_blank" class="font-weight-bold">SAC (Soft Actor-Critic) - Alpha Tuning</a> <span class="badge badge-success">OFF-POLICY</span>
                                                            </td>
                                                            <td>
{#                                                                <span class="font-weight-bold">Homework #3</span>. 자신의 연구와 관련된 환경 또는 흥미있는 환경을 하나 정하고 해당 환경에 SAC 기반 강화학습 코딩 및 실험 결과 담은 리포트 제출#}
                                                            </td>
                                                        </tr>
                                                        <tr>
                                                            <th scope="row">15</th>
                                                            <td>06월 14일(화)</td>
                                                            <td COLSPAN="2">기말 고사</td>
                                                            <td>
                                                            </td>
                                                        </tr>
                                                        </tbody>
                                                    </table>
                                                    <br/>
                                                    <h3 class="title-level-3">2. Course Information</h3>
                                                    <br/>
                                                    <ul>
                                                        <li>
                                                            - Lecturer: 한연희 교수 (Rm. 2공학관 423호, Email: yhhan@koreatech.ac.kr)
                                                        </li>
                                                        <li>
                                                            - Classes: 화요일 (19:00 ~ 21:50, 11A ~ 13B)
                                                        </li>
                                                        <li>
                                                            - Lecture Room: 409호
                                                        </li>
                                                        <li>
                                                            - Prerequisites: 머신러닝 및 딥러닝 기본 지식, PyTorch/Tensorflow 기본 코딩 경험
                                                        </li>
                                                    </ul>
                                                    <br/>
                                                    <h3 class="title-level-3">3. Paper Presentation Evaluation</h3>
                                                    <br/>
                                                    <ul>
                                                        <li>
                                                            - 내용 이해도 (50%), 발표 자료 충실도 (30%), 발표 역량 (20%)
                                                        </li>
                                                    </ul>
                                                    <br/>
                                                    <h3 class="title-level-3">4. Home Work Guide</h3>
                                                    <br/>
                                                    <ul>
                                                        <li>
                                                            - 추후 구체적인 코딩 가이드 라인 제공
                                                        </li>
                                                        <li>
                                                            - PyTorch/Tensorflow 기반 강화학습 코딩 및 실험 결과 담은 리포트 제출
                                                        </li>
                                                    </ul>
                                                    <br/>
                                                    <h3 class="title-level-3">5. References</h3>
                                                    <br/>
                                                    <div class="row">
                                                        <div class="col-12">
                                                            <h4>[주교재]</h4>
                                                            <ul>
                                                                <li>- 수업 시간 PDF로 제공</li>
                                                                <li>- 심층강화학습 핵심 논문</li>
                                                            </ul>
                                                            <h4>[부교재]</h4>
                                                            <ul>
                                                                <li>
                                                                    - 심층 강화학습 인 액션 : 기본 개념부터 파이썬 기반의 최신 알고리즘 구현까지
                                                                    <a href="http://www.kyobobook.co.kr/product/detailViewKor.laf?mallGb=KOR&ejkGb=KOR&barcode=9791190665612" target="_blank">
                                                                        <i class="fa fa-link" aria-hidden="true"></i>
                                                                    </a>
                                                                </li>
                                                                <li>
                                                                    - 파이썬 기반 강화학습 알고리듬 DP, Q-Learning, AC, DQN, TRPO, PPO, DDPG, TD3 | Imitation Learning, ESBAS 알아보기
                                                                    <a href="http://www.kyobobook.co.kr/product/detailViewKor.laf?ejkGb=KOR&mallGb=KOR&barcode=9791161755571&orderClick=LAH&Kc=#N" target="_blank">
                                                                        <i class="fa fa-link" aria-hidden="true"></i>
                                                                    </a>
                                                                </li>
                                                                <li>
                                                                    - 심층강화학습 주요 논문 모음: <a href="https://spinningup.openai.com/en/latest/spinningup/keypapers.html" target="_blank" class="font-weight-bold"><i class="fa fa-link" aria-hidden="true"></i></a>
                                                                </li>
                                                                <li>
                                                                    - PyTorch 튜토리얼: <a href="https://github.com/MorvanZhou/PyTorch-Tutorial" target="_blank" class="font-weight-bold"><i class="fa fa-link" aria-hidden="true"></i></a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </div>
                                                    <br/>
                                                    <h3 class="title-level-3">6. Logistics</h3>
                                                    <br/>
                                                    <ul>
                                                        <li>- <span class="font-weight-bold">Attendance</span>: one class absence will result in the deduction
                                                            of two points out of 100 points. Five absences will not
                                                            result in ten points deduction, but “failure” (i.e., grade
                                                            ‘F’) in this course.
                                                        </li>
                                                        <li>- <span class="font-weight-bold">Homework</span>: much intensive homework will be set. Any cheating
                                                            (or copying) will result in grade ‘F’.
                                                        </li>
                                                        <li>- <span class="font-weight-bold">Exam</span>: there will be the final examination for the
                                                            evaluation of the knowledge learned from the class.
                                                        </li>
                                                    </ul>
                                                    <br>
                                                    <h3 class="title-level-3">7. Lecture Evaluation</h3>
                                                    <br/>
                                                    <ul>
                                                        <li>Attendance (10%), Paper Presentation (20%), Homework #1/#2/#3 Reports (30%), Final Exam. (40%)</li>
                                                    </ul>
                                                </div>
                                            </div>
                                            <div class="clear"></div>
                                            <!--End Content Grid-->
                                        </div>
                                    </div>
                                </div>
                                <!--End Container Div-->
                            </div>
                        </div>
                    </div>
                    <div class="clear"></div>
                    <!--End Content Grid-->
                </div>
            </div>
        </div>
        <!--End Container Div-->
    </div>
</section>